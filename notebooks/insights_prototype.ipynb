{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cassidy Insights Prototype\n",
    "\n",
    "This notebook allows you to prototype insights services that will eventually run as cron jobs.\n",
    "Use this to test different prompts and analysis approaches on user data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Database initialized\n",
      "✅ Imports successful\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "# Add backend to Python path\n",
    "sys.path.append('../backend')\n",
    "sys.path.append('../backend/app')\n",
    "\n",
    "# Set database URL to use backend database\n",
    "os.environ['DATABASE_URL'] = 'sqlite+aiosqlite:///../backend/cassidy.db'\n",
    "\n",
    "# Backend imports\n",
    "from app.database import init_db, get_db\n",
    "from app.repositories.user import UserRepository\n",
    "from app.repositories.task import TaskRepository\n",
    "from app.repositories.session import JournalEntryRepository\n",
    "from app.agents.factory import AgentFactory\n",
    "from app.agents.models import CassidyAgentDependencies\n",
    "from app.core.config import settings\n",
    "\n",
    "# Initialize database\n",
    "await init_db()\n",
    "print(\"✅ Database initialized\")\n",
    "\n",
    "# Create a helper for database sessions\n",
    "@asynccontextmanager\n",
    "async def get_db_session():\n",
    "    \"\"\"Helper to get database session\"\"\"\n",
    "    async for session in get_db():\n",
    "        yield session\n",
    "\n",
    "print(\"✅ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing data for user: jg2950\n",
      "Looking back 100 days\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "USERNAME = \"jg2950\"  # Change this to test with different users\n",
    "DAYS_BACK = 100  # How many days of data to analyze\n",
    "\n",
    "print(f\"Analyzing data for user: {USERNAME}\")\n",
    "print(f\"Looking back {DAYS_BACK} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def debug_user_lookup(username: str):\n",
    "    \"\"\"Debug user lookup to see what's happening\"\"\"\n",
    "    async with get_db_session() as db:\n",
    "        user_repo = UserRepository()\n",
    "        \n",
    "        # First, try to get user without any filters\n",
    "        print(f\"Looking for user: {username}\")\n",
    "        \n",
    "        # Raw SQL check\n",
    "        from sqlalchemy import text\n",
    "        result = await db.execute(text(\"SELECT username, id, is_active FROM users WHERE username = :username\"), \n",
    "                                {\"username\": username})\n",
    "        raw_result = result.fetchone()\n",
    "        print(f\"Raw SQL result: {raw_result}\")\n",
    "        \n",
    "        # Try the actual repository method\n",
    "        user = await user_repo.get_by_username(db, username)\n",
    "        print(f\"Repository result: {user}\")\n",
    "        \n",
    "        return user\n",
    "\n",
    "# Debug the user lookup\n",
    "test_user = await debug_user_lookup(\"jg2950\")\n",
    "\n",
    "async def load_user_data(username: str):\n",
    "    \"\"\"Load all user data including preferences, journals, and tasks\"\"\"\n",
    "    \n",
    "    async with get_db_session() as db:\n",
    "        user_repo = UserRepository()\n",
    "        task_repo = TaskRepository()\n",
    "        journal_repo = JournalEntryRepository()\n",
    "        \n",
    "        # Get user\n",
    "        user = await user_repo.get_by_username(db, username)\n",
    "        if not user:\n",
    "            print(f\"⚠️  User {username} not found in database.\")\n",
    "            print(\"Create the user first by:\")\n",
    "            print(\"1. Running the backend server to initialize the database\")\n",
    "            print(\"2. Using the frontend to register the user\")\n",
    "            print(\"3. Or use the default test user 'user_123'\")\n",
    "            raise ValueError(f\"User {username} not found\")\n",
    "        \n",
    "        user_id = str(user.id)\n",
    "        \n",
    "        # Get tasks\n",
    "        tasks = await task_repo.get_by_user_id(db, user_id)\n",
    "        \n",
    "        # Get journal entries\n",
    "        journals = await journal_repo.get_by_user_id(db, user_id)\n",
    "        \n",
    "        return {\n",
    "            'user': user,\n",
    "            'user_id': user_id,\n",
    "            'tasks': tasks,\n",
    "            'journals': journals\n",
    "        }\n",
    "\n",
    "def format_data_summary(data):\n",
    "    \"\"\"Print a summary of loaded data\"\"\"\n",
    "    print(f\"User: {data['user'].username} ({data['user_id']})\")\n",
    "    print(f\"Tasks: {len(data['tasks'])} total\")\n",
    "    print(f\"Journal entries: {len(data['journals'])} total\")\n",
    "    \n",
    "    # Task breakdown\n",
    "    completed_tasks = [t for t in data['tasks'] if t.is_completed]\n",
    "    pending_tasks = [t for t in data['tasks'] if not t.is_completed]\n",
    "    print(f\"  - Completed: {len(completed_tasks)}\")\n",
    "    print(f\"  - Pending: {len(pending_tasks)}\")\n",
    "    \n",
    "    # Journal breakdown\n",
    "    recent_journals = [j for j in data['journals'] if j.created_at > datetime.now() - timedelta(days=DAYS_BACK)]\n",
    "    print(f\"  - Recent ({DAYS_BACK} days): {len(recent_journals)}\")\n",
    "    \n",
    "    return recent_journals\n",
    "\n",
    "async def create_sample_data_if_needed():\n",
    "    \"\"\"Create sample user and data if none exists\"\"\"\n",
    "    from app.database import create_sample_user\n",
    "    \n",
    "    try:\n",
    "        await create_sample_user()\n",
    "        print(\"✅ Sample user created or already exists\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create sample user: {e}\")\n",
    "        \n",
    "# Create sample user if needed (useful for first-time setup)\n",
    "await create_sample_data_if_needed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load User Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  User jg2950 not found in database.\n",
      "Create the user first by:\n",
      "1. Running the backend server to initialize the database\n",
      "2. Using the frontend to register the user\n",
      "3. Or use the default test user 'user_123'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "User jg2950 not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load data for the specified user\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m user_data = \u001b[38;5;28;01mawait\u001b[39;00m load_user_data(USERNAME)\n\u001b[32m      3\u001b[39m recent_journals = format_data_summary(user_data)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ Data loaded successfully\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mload_user_data\u001b[39m\u001b[34m(username)\u001b[39m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m2. Using the frontend to register the user\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     40\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m3. Or use the default test user \u001b[39m\u001b[33m'\u001b[39m\u001b[33muser_123\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUser \u001b[39m\u001b[38;5;132;01m{\u001b[39;00musername\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m user_id = \u001b[38;5;28mstr\u001b[39m(user.id)\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Get tasks\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: User jg2950 not found"
     ]
    }
   ],
   "source": [
    "# Load data for the specified user\n",
    "user_data = await load_user_data(USERNAME)\n",
    "recent_journals = format_data_summary(user_data)\n",
    "\n",
    "print(\"\\n✅ Data loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "async def get_llm_agent():\n    \"\"\"Get an LLM agent for analysis\"\"\"\n    # Create minimal dependencies for the agent\n    deps = CassidyAgentDependencies(\n        user_id=user_data['user_id'],\n        session_id=\"notebook-session\",\n        conversation_type=\"general\",\n        user_template={},\n        user_preferences={},\n        current_journal_draft={},\n        current_tasks=[]\n    )\n    \n    # Create agent factory and get a general agent\n    agent = await AgentFactory.get_agent(\n        conversation_type=\"general\",\n        user_id=user_data['user_id'],\n        context=deps\n    )\n    \n    return agent\n\nasync def ask_llm(prompt: str, agent=None):\n    \"\"\"Send a prompt to the LLM and get response\"\"\"\n    if agent is None:\n        agent = await get_llm_agent()\n    \n    deps = CassidyAgentDependencies(\n        user_id=user_data['user_id'],\n        session_id=\"notebook-session\",\n        conversation_type=\"general\",\n        user_template={},\n        user_preferences={},\n        current_journal_draft={},\n        current_tasks=[]\n    )\n    \n    result = await agent.run(prompt, deps=deps)\n    return result.output\n\n# Initialize agent\nllm_agent = await get_llm_agent()\nprint(\"✅ LLM agent initialized\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotional State Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_journal_text(journal_entry):\n",
    "    \"\"\"Extract readable text from journal entry\"\"\"\n",
    "    if hasattr(journal_entry, 'raw_text') and journal_entry.raw_text:\n",
    "        return journal_entry.raw_text\n",
    "    elif hasattr(journal_entry, 'structured_data') and journal_entry.structured_data:\n",
    "        data = journal_entry.structured_data if isinstance(journal_entry.structured_data, dict) else json.loads(journal_entry.structured_data)\n",
    "        return data.get('content', '') or data.get('raw_text', '')\n",
    "    return \"No content available\"\n",
    "\n",
    "async def rate_emotional_state(journal_text: str, agent=None) -> int:\n",
    "    \"\"\"Rate the emotional state of a journal entry from 1-5\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Rate the emotional state in this journal entry on a scale of 1-5, where:\n",
    "1 = Very negative (depression, anger, despair, anxiety)\n",
    "2 = Somewhat negative (frustration, sadness, worry)\n",
    "3 = Neutral (matter-of-fact, balanced)\n",
    "4 = Somewhat positive (content, hopeful, calm)\n",
    "5 = Very positive (joy, excitement, gratitude, love)\n",
    "\n",
    "Journal entry:\n",
    "\"{journal_text}\"\n",
    "\n",
    "Respond with ONLY the number (1-5), no explanation.\n",
    "\"\"\"\n",
    "    \n",
    "    response = await ask_llm(prompt, agent)\n",
    "    \n",
    "    # Extract number from response\n",
    "    try:\n",
    "        rating = int(response.strip())\n",
    "        return max(1, min(5, rating))  # Clamp to 1-5 range\n",
    "    except ValueError:\n",
    "        # Fallback: look for digit in response\n",
    "        import re\n",
    "        match = re.search(r'[1-5]', response)\n",
    "        return int(match.group()) if match else 3  # Default to neutral\n",
    "\n",
    "async def analyze_journals_emotional_state(journals: List, agent=None):\n",
    "    \"\"\"Analyze emotional state for all journal entries\"\"\"\n",
    "    print(f\"Analyzing emotional state for {len(journals)} journal entries...\")\n",
    "    \n",
    "    analyzed_journals = []\n",
    "    \n",
    "    for i, journal in enumerate(journals):\n",
    "        print(f\"Processing journal {i+1}/{len(journals)}...\", end=\" \")\n",
    "        \n",
    "        text = extract_journal_text(journal)\n",
    "        if len(text.strip()) < 10:  # Skip very short entries\n",
    "            print(\"(skipped - too short)\")\n",
    "            continue\n",
    "            \n",
    "        rating = await rate_emotional_state(text, agent)\n",
    "        \n",
    "        analyzed_journals.append({\n",
    "            'journal': journal,\n",
    "            'text': text,\n",
    "            'emotional_rating': rating,\n",
    "            'date': journal.created_at.strftime('%Y-%m-%d'),\n",
    "            'structured_data': journal.structured_data if hasattr(journal, 'structured_data') else None\n",
    "        })\n",
    "        \n",
    "        print(f\"Rating: {rating}\")\n",
    "    \n",
    "    return analyzed_journals\n",
    "\n",
    "print(\"✅ Emotional analysis functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Emotional State Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze emotional state for recent journals\n",
    "analyzed_journals = await analyze_journals_emotional_state(recent_journals, llm_agent)\n",
    "\n",
    "# Show distribution\n",
    "ratings = [j['emotional_rating'] for j in analyzed_journals]\n",
    "print(f\"\\nEmotional state distribution:\")\n",
    "for rating in range(1, 6):\n",
    "    count = ratings.count(rating)\n",
    "    percentage = (count / len(ratings) * 100) if ratings else 0\n",
    "    print(f\"  {rating}: {count} entries ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\n✅ Analyzed {len(analyzed_journals)} journal entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative State Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for low emotional state entries (rating < 3)\n",
    "negative_journals = [j for j in analyzed_journals if j['emotional_rating'] < 3]\n",
    "\n",
    "print(f\"Found {len(negative_journals)} journal entries with negative emotional state:\")\n",
    "for journal in negative_journals:\n",
    "    print(f\"  - {journal['date']}: Rating {journal['emotional_rating']} - {journal['text'][:100]}...\")\n",
    "\n",
    "if len(negative_journals) == 0:\n",
    "    print(\"\\n⚠️ No negative emotional state entries found. Consider adjusting the rating threshold or date range.\")\n",
    "else:\n",
    "    print(f\"\\n✅ Ready to analyze {len(negative_journals)} negative state entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def find_negative_correlations(negative_entries: List[Dict], agent=None):\n",
    "    \"\"\"Find patterns that correlate with negative emotional states\"\"\"\n",
    "    \n",
    "    if not negative_entries:\n",
    "        return \"No negative emotional state entries to analyze.\"\n",
    "    \n",
    "    # Prepare structured data for analysis\n",
    "    analysis_data = []\n",
    "    for entry in negative_entries:\n",
    "        entry_info = {\n",
    "            'date': entry['date'],\n",
    "            'emotional_rating': entry['emotional_rating'],\n",
    "            'text': entry['text'][:500],  # Limit text length\n",
    "        }\n",
    "        \n",
    "        # Include structured data if available\n",
    "        if entry['structured_data']:\n",
    "            try:\n",
    "                structured = entry['structured_data'] if isinstance(entry['structured_data'], dict) else json.loads(entry['structured_data'])\n",
    "                entry_info['structured_data'] = structured\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        analysis_data.append(entry_info)\n",
    "    \n",
    "    # Create analysis prompt\n",
    "    prompt = f\"\"\"\n",
    "You are analyzing journal entries to find patterns that correlate with negative emotional states.\n",
    "\n",
    "I have {len(negative_entries)} journal entries where the person was experiencing negative emotions (rated 1-2 out of 5).\n",
    "\n",
    "Journal entries with negative emotional states:\n",
    "{json.dumps(analysis_data, indent=2)}\n",
    "\n",
    "Please analyze these entries and identify:\n",
    "\n",
    "1. **Recurring themes or topics** that appear in multiple negative entries\n",
    "2. **Environmental factors** (time of day, weather, location, etc.) that might correlate\n",
    "3. **Activities or behaviors** mentioned that might be contributing factors\n",
    "4. **Relationship or social patterns** that appear problematic\n",
    "5. **Work or life stressors** that repeatedly come up\n",
    "\n",
    "Focus on finding patterns that are:\n",
    "- **Actionable** (things the person could potentially change)\n",
    "- **Recurring** (appear in multiple entries, not just isolated incidents)\n",
    "- **Likely causal** (not just coincidental)\n",
    "\n",
    "Format your response as a clear analysis with specific examples from the journal entries.\n",
    "\"\"\"\n",
    "    \n",
    "    return await ask_llm(prompt, agent)\n",
    "\n",
    "# Run the correlation analysis\n",
    "if negative_journals:\n",
    "    print(\"Analyzing patterns in negative emotional states...\")\n",
    "    correlation_analysis = await find_negative_correlations(negative_journals, llm_agent)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"NEGATIVE EMOTIONAL STATE CORRELATION ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    print(correlation_analysis)\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"No negative journal entries to analyze.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Insights Experimentation\n",
    "\n",
    "Use the cells below to experiment with different insights prompts and approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Task prioritization insight\n",
    "async def prioritize_tasks_insight(tasks: List, journals: List, agent=None):\n",
    "    \"\"\"Generate insights about task prioritization based on journal context\"\"\"\n",
    "    \n",
    "    pending_tasks = [t for t in tasks if not t.is_completed]\n",
    "    \n",
    "    if not pending_tasks:\n",
    "        return \"No pending tasks to prioritize.\"\n",
    "    \n",
    "    # Recent journal themes\n",
    "    recent_journal_text = \"\\n\".join([extract_journal_text(j)[:200] for j in journals[-5:]])\n",
    "    \n",
    "    task_list = \"\\n\".join([f\"- {t.title}: {t.description or 'No description'}\" for t in pending_tasks])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Based on this person's recent journal entries and current tasks, provide insights on task prioritization.\n",
    "\n",
    "Recent journal themes:\n",
    "{recent_journal_text}\n",
    "\n",
    "Pending tasks:\n",
    "{task_list}\n",
    "\n",
    "Provide:\n",
    "1. Top 3 tasks to prioritize this week based on their current state of mind and concerns\n",
    "2. Tasks that might be causing stress and should be addressed or delegated\n",
    "3. Tasks that align with their current emotional needs and goals\n",
    "\n",
    "Focus on actionable, personalized recommendations.\n",
    "\"\"\"\n",
    "    \n",
    "    return await ask_llm(prompt, agent)\n",
    "\n",
    "# Run task prioritization insight\n",
    "print(\"Generating task prioritization insights...\")\n",
    "task_insights = await prioritize_tasks_insight(user_data['tasks'], recent_journals, llm_agent)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TASK PRIORITIZATION INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "print(task_insights)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Weekly reflection and goal setting\n",
    "async def weekly_reflection_insight(journals: List, tasks: List, agent=None):\n",
    "    \"\"\"Generate a weekly reflection and goal-setting insight\"\"\"\n",
    "    \n",
    "    # Get this week's journals\n",
    "    week_ago = datetime.now() - timedelta(days=7)\n",
    "    weekly_journals = [j for j in journals if j.created_at > week_ago]\n",
    "    \n",
    "    # Get completed tasks this week\n",
    "    completed_this_week = [t for t in tasks if t.is_completed and t.updated_at and t.updated_at > week_ago]\n",
    "    \n",
    "    journal_summaries = \"\\n\".join([\n",
    "        f\"Day {j.created_at.strftime('%A')}: {extract_journal_text(j)[:150]}...\"\n",
    "        for j in weekly_journals[-7:]  # Last 7 entries\n",
    "    ])\n",
    "    \n",
    "    completed_tasks = \"\\n\".join([f\"- {t.title}\" for t in completed_this_week])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Provide a weekly reflection and goal-setting insight based on this person's journal entries and accomplishments.\n",
    "\n",
    "This week's journal entries:\n",
    "{journal_summaries}\n",
    "\n",
    "Tasks completed this week:\n",
    "{completed_tasks}\n",
    "\n",
    "Provide:\n",
    "1. **Key themes and patterns** from the week\n",
    "2. **Wins and accomplishments** to celebrate\n",
    "3. **Areas for improvement** or attention\n",
    "4. **3 specific goals** for next week based on their patterns and needs\n",
    "5. **One self-care recommendation** based on their emotional state\n",
    "\n",
    "Make it personal, encouraging, and actionable.\n",
    "\"\"\"\n",
    "    \n",
    "    return await ask_llm(prompt, agent)\n",
    "\n",
    "# Run weekly reflection\n",
    "print(\"Generating weekly reflection insights...\")\n",
    "weekly_insights = await weekly_reflection_insight(recent_journals, user_data['tasks'], llm_agent)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WEEKLY REFLECTION & GOAL SETTING\")\n",
    "print(\"=\"*80)\n",
    "print(weekly_insights)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with Custom Prompts\n",
    "\n",
    "Use this cell to test your own insight prompts and analysis approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom experiment space\n",
    "# Example: Find productivity patterns\n",
    "\n",
    "custom_prompt = f\"\"\"\n",
    "# Add your custom prompt here\n",
    "# You have access to:\n",
    "# - user_data['tasks'] - all tasks\n",
    "# - recent_journals - journal entries from last {DAYS_BACK} days\n",
    "# - analyzed_journals - journals with emotional ratings\n",
    "# - negative_journals - journals with negative emotional states\n",
    "\n",
    "# Example experiment:\n",
    "Analyze this person's productivity patterns. What times of day, activities, or conditions \n",
    "seem to correlate with task completion and positive outcomes?\n",
    "\n",
    "Recent tasks completed: {len([t for t in user_data['tasks'] if t.is_completed])}\n",
    "Recent journal entries: {len(recent_journals)}\n",
    "\"\"\"\n",
    "\n",
    "# Run your custom analysis\n",
    "custom_result = await ask_llm(custom_prompt, llm_agent)\n",
    "print(\"CUSTOM ANALYSIS RESULT:\")\n",
    "print(\"=\"*50)\n",
    "print(custom_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Export for Further Analysis\n",
    "\n",
    "Export processed data for use in other tools or deeper analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export analyzed data\n",
    "export_data = {\n",
    "    'user_id': user_data['user_id'],\n",
    "    'username': user_data['user'].username,\n",
    "    'analysis_date': datetime.now().isoformat(),\n",
    "    'date_range_days': DAYS_BACK,\n",
    "    'summary': {\n",
    "        'total_tasks': len(user_data['tasks']),\n",
    "        'completed_tasks': len([t for t in user_data['tasks'] if t.is_completed]),\n",
    "        'total_journals': len(recent_journals),\n",
    "        'analyzed_journals': len(analyzed_journals),\n",
    "        'negative_state_journals': len(negative_journals)\n",
    "    },\n",
    "    'emotional_distribution': {str(i): ratings.count(i) for i in range(1, 6)} if ratings else {},\n",
    "    'analyzed_journals': [{\n",
    "        'date': j['date'],\n",
    "        'emotional_rating': j['emotional_rating'],\n",
    "        'text_preview': j['text'][:200]\n",
    "    } for j in analyzed_journals]\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "export_filename = f\"insights_analysis_{USERNAME}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(export_filename, 'w') as f:\n",
    "    json.dump(export_data, f, indent=2)\n",
    "\n",
    "print(f\"✅ Analysis data exported to: {export_filename}\")\n",
    "print(f\"Summary: {len(analyzed_journals)} journals analyzed, {len(negative_journals)} with negative emotional states\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}